include: "common.smk"


localrules:
    filter_vep,
    final_bamfile,


rule all:
    input:
        bam=[module_output.bam(sample) for sample in samples],
        bai=[module_output.bai(sample) for sample in samples],
        filter_vep=[module_output.filter_vep(sample) for sample in samples],
        json=[module_output.json(sample) for sample in samples],
        hotspot=[module_output.hotspot(sample) for sample in samples],
        multiqc="multiqc_snv_indels.html",


rule tmpdir:
    output:
        temporary(directory("snv-indel-tmp")),
    container:
        containers["vardict"]
    log:
        "log/tmpdir.txt",
    shell:
        """
        mkdir -p {output} 2> {log}
        """


rule call_regions:
    """Determine the regions to call from the GTF file"""
    input:
        fasta=config["genome_fasta"],
        gtf=config["gtf"],
        src=workflow.source_path("scripts/create_bed.sh"),
    output:
        # Intermediate files from the bash script
        chroms=temporary("chroms.txt"),
        all_genes=temporary("all_genes.bed"),
        # Final bed file
        bed=temporary("call_regions.bed"),
    log:
        "log/call_regions.txt",
    container:
        containers["bedtools-2.27-grep-2.14-gawk-5.0-python-3.7"]
    shell:
        """
        bash {input.src} {input.fasta} {input.gtf} > {output.bed} 2> {log}
        """


rule align_vars:
    input:
        fq1=get_forward,
        fq2=get_reverse,
        index=config["star_index"],
        gtf=config["gtf"],
    output:
        bam="{sample}/snv-indels/{sample}.bam",
        gene_count="{sample}/snv-indels/{sample}.ReadsPerGene.out.tab",
    params:
        rg_sample=lambda wildcards: wildcards.sample,
        chim_segment=20,
        min_intron_size=50,
        alignInsertionFlush="Right",
        twopassMode="Basic",
        outBAMsortingBinsN=100,
    log:
        main="{sample}/snv-indels/Log.out",
        progress="{sample}/snv-indels/Log.progress.out",
        final="{sample}/snv-indels/Log.final.out",
    threads: 8
    container:
        containers["star"]
    shell:
        """
        STAR \
            --runThreadN {threads} \
            --genomeDir {input.index} \
            --sjdbGTFfile {input.gtf} \
            --readFilesCommand zcat \
            --outSAMattrRGline "ID:{params.rg_sample}" "SM:{params.rg_sample}" \
            --outFileNamePrefix $(dirname {output.bam})/ \
            --outSAMtype BAM SortedByCoordinate \
            --outSAMunmapped Within \
            --alignIntronMin {params.min_intron_size} \
            --alignInsertionFlush {params.alignInsertionFlush} \
            --twopassMode {params.twopassMode} \
            --chimOutType WithinBAM \
            --chimSegmentMin {params.chim_segment} \
            --quantMode GeneCounts \
            --outBAMsortingBinsN {params.outBAMsortingBinsN} \
            --readFilesIn {input.fq1:q} {input.fq2:q}

        # Rename the count file to include the sample name
        mv {wildcards.sample}/snv-indels/ReadsPerGene.out.tab {output.gene_count}

        # Rename the bam file
        mv {wildcards.sample}/snv-indels/Aligned.sortedByCoord.out.bam {output.bam}

        # Remove temporary files that STAR leaves all over the place
        rm -rf {wildcards.sample}/snv-indels/_STAR*
        """


rule index_bamfile:
    input:
        bam="{sample}/snv-indels/{sample}.bam",
        tmp=rules.tmpdir.output,
    output:
        bai="{sample}/snv-indels/{sample}.bam.bai",
    log:
        "log/index_bamfile.{sample}.txt",
    container:
        containers["varscan-2.4.2-samtools-1.3.1-tabix-0.2.6-grep-2.14"]
    shell:
        """
        samtools index {input.bam} 2> {log}
        """


rule genome_txt:
    input:
        ref_dict=config["genome_dict"],
    output:
        genome=temp(".tmp.genome.txt"),
    log:
        "log/tmp.genome.txt",
    container:
        containers["varscan-2.4.2-samtools-1.3.1-tabix-0.2.6-grep-2.14"]
    shell:
        """
        cat {input.ref_dict} \
        | grep -P "@SQ\\tSN:" \
        | sed 's/@SQ\\tSN://' \
        | sed 's/\\tLN:/\\t/' \
        | cut -f1,2 \
        > {output.genome} 2> {log}
        """


rule exon_cov_ref:
    input:
        ref_fai=config["genome_fai"],
        ref_refflat=config["annotation_refflat"],
    output:
        bed=temp(".tmp.exon_cov_ref.bed"),
    log:
        "log/exon_cov_ref.txt",
    container:
        containers["bedtools-2.27-grep-2.14-gawk-5.0-python-3.7"]
    shell:
        """
        cat {input.ref_refflat} \
         | grep -vP "chr.*(alt|random|fix)\t" \
         | awk \'{{ split($10, starts, ","); split($11, ends, ","); for (i=1; i < length(starts); i++) {{ print $3"\\t"starts[i]"\\t"ends[i]"\\t"gensub(/(\.[0-9]+)/,"", "g", $2)"\\t"i"\\t"$4 }} }}\' \
         | bedtools sort -faidx {input.ref_fai} \
         > {output.bed} 2> {log}
        """


rule exon_cov:
    input:
        bam=module_output.bam,
        bai=module_output.bai,
        bed=".tmp.exon_cov_ref.bed",
        genome=".tmp.genome.txt",
        idm=config["ref_id_mapping"],
        scr=workflow.source_path("scripts/aggr_exon_cov.py"),
    output:
        json="{sample}/snv-indels/{sample}.exon_cov_stats.json",
    log:
        "log/exon_cov.{sample}.txt",
    container:
        containers["bedtools-2.27-grep-2.14-gawk-5.0-python-3.7"]
    shell:
        """
        bedtools coverage \
            -d \
            -sorted \
            -g {input.genome} \
            -a {input.bed} \
            -b {input.bam} 2> {log} \
        | cut -f1,2,3,4,5,8,7 \
        | python {input.scr} \
            --id-mapping {input.idm} - {output.json} 2> {log}
        """


rule call_vars:
    input:
        bam=module_output.bam,
        bai=module_output.bai,
        ref=config["genome_fasta"],
        bed=rules.call_regions.output.bed,
        tmp=rules.tmpdir.output,
    output:
        vcf="{sample}/snv-indels/{sample}.raw.vcf.gz",
    params:
        min_af=0.05,
        bed_chrom=1,
        bed_start=2,
        bed_end=3,
        bed_gene=4,
        min_sv_len=10_000_000,
    log:
        vardict="log/vardict.{sample}.txt",
        strandbias="log/strandbias.{sample}.txt",
        var2vcf="log/var2vcf.{sample}.txt",
        gzip="log/gzip.{sample}.txt",
    threads: 8
    container:
        containers["vardict"]
    shell:
        """
        export TMPDIR={input.tmp}

        vardict-java \
            -G {input.ref} \
            -N {wildcards.sample} \
            -b {input.bam} \
            -f {params.min_af} \
            -c {params.bed_chrom} \
            -S {params.bed_start} \
            -E {params.bed_end} \
            -g {params.bed_gene} \
            -L {params.min_sv_len} \
            -th {threads} \
            --verbose \
            {input.bed} 2> {log.vardict} \
            |
        teststrandbias.R 2> {log.strandbias} \
            |
        var2vcf_valid.pl -A 2> {log.var2vcf} \
            |
        gzip > {output.vcf} 2> {log.gzip}
        """


rule picard_metrics:
    input:
        bam=module_output.bam,
        bai=module_output.bai,
        ref=config["genome_fasta"],
        ref_dict=config["genome_dict"],
        ref_rrna=config["rrna_refflat"],
        annot=config["annotation_refflat"],
    output:
        alignment="{sample}/snv-indels/{sample}.alignment_summary_metrics",
        rna_seq="{sample}/snv-indels/{sample}.rna_metrics",
        insert_size="{sample}/snv-indels/{sample}.insert_size_metrics",
    log:
        "log/picard_metrics.{sample}.txt",
    threads: 1
    container:
        containers["picard"]
    shell:
        """
        picard -Xmx4G CollectMultipleMetrics \
            PROGRAM=null \
            PROGRAM=CollectAlignmentSummaryMetrics \
            PROGRAM=CollectInsertSizeMetrics \
            PROGRAM=RnaSeqMetrics \
            VALIDATION_STRINGENCY=LENIENT \
            R={input.ref} \
            I={input.bam} \
            REF_FLAT={input.annot} \
            EXTRA_ARGUMENT="RnaSeqMetrics::RIBOSOMAL_INTERVALS={input.ref_rrna}" \
            USE_JDK_DEFLATER=true \
            USE_JDK_INFLATER=true \
            O={wildcards.sample}/snv-indels/{wildcards.sample} 2> {log}

        """


rule annotate_vars:
    """Annotate variants using VEP"""
    input:
        vcf="{sample}/snv-indels/{sample}.raw.vcf.gz",
        genome_fasta=config["genome_fasta"],
        gtf=config["gtf"],
    params:
        vep_cache=config.get("vep_cache", ""),
    output:
        vep="{sample}/snv-indels/{sample}.vep.txt.gz",
        stats="{sample}/snv-indels/{sample}.vep_stats.txt",
    log:
        "log/annotate_vars.{sample}.txt",
    threads: 8
    container:
        containers["vep"]
    shell:
        """

        # Run from the GTF file if the vep_cache is not defined
        if [ -z "{params.vep_cache}" ]; then

          # Prepare the GTF file for use by VEP
          gtf={wildcards.sample}/snv-indels/vep.gtf
          grep -v "#" {input.gtf} | sort -k1,1 -k4,4n -k5,5n -t$'\t' | bgzip -c > ${{gtf}}.gz
          tabix ${{gtf}}.gz

          flags="--gtf ${{gtf}}.gz"
        else
          flags="--offline --dir {params.vep_cache} --hgvsg_use_accession"
        fi

        vep \
            -i {input.vcf} \
            --fasta {input.genome_fasta} \
            ${{flags}} \
            --verbose \
            --fork {threads} \
            --allele_number \
            --stats_text \
            --json \
            --force_overwrite \
            --format vcf \
            --polyphen b \
            --sift b \
            --hgvs \
            --hgvsg \
            --numbers \
            --merged \
            --everything \
            --stats_file {output.stats} \
            --output_file STDOUT | gzip > {output.vep} \
            2> {log}
        """


rule hotspot_variants:
    input:
        vcf="{sample}/snv-indels/{sample}.raw.vcf.gz",
        bed=config["bed_variant_hotspots"],
    output:
        vcf="{sample}/snv-indels/{sample}.hotspot.vcf",
    log:
        "log/hotspot_variants.{sample}.txt",
    threads: 1
    container:
        containers["bedtools-2.27-grep-2.14-gawk-5.0-python-3.7"]
    shell:
        """
        bedtools intersect \
            -a {input.vcf} \
            -b {input.bed} \
            -header \
            > {output.vcf} 2> {log}
        """


rule filter_vep:
    input:
        vep="{sample}/snv-indels/{sample}.vep.txt.gz",
        ref_id_mapping=config["ref_id_mapping"],
        hotspots="{sample}/snv-indels/{sample}.hotspot.vcf",
        scr=workflow.source_path("scripts/filter_vep.py"),
        blacklist=config.get("blacklist", []),
    params:
        vep_consequences=config["vep_include_consequence"],
        blacklist=f"--blacklist {config['blacklist']}" if "blacklist" in config else "",
        population="gnomade",
        max_pop_af=0.01,
    output:
        filtered="{sample}/snv-indels/{sample}.vep.filtered.txt.gz",
    log:
        "log/filter_vep.{sample}.txt",
    threads: 1
    container:
        containers["crimson"]
    shell:
        """
        python3 {input.scr} \
            {input.vep} \
            {input.ref_id_mapping} \
            --hotspot {input.hotspots} \
            --consequences {params.vep_consequences} \
            --population {params.population} \
            --frequency {params.max_pop_af} \
            {params.blacklist} \
            | gzip > {output.filtered} 2> {log}
        """


rule json_output:
    input:
        id_mappings_path=config["ref_id_mapping"],
        filter_vep=module_output.filter_vep,
        aln_stats="{sample}/snv-indels/{sample}.alignment_summary_metrics",
        rna_stats="{sample}/snv-indels/{sample}.rna_metrics",
        insert_stats="{sample}/snv-indels/{sample}.insert_size_metrics",
        exon_cov_stats="{sample}/snv-indels/{sample}.exon_cov_stats.json",
        vep_stats="{sample}/snv-indels/{sample}.vep_stats.txt",
        src=workflow.source_path("scripts/json-output.py"),
    output:
        "{sample}/snv-indels/snv-indels-output.json",
    log:
        "log/snv_indels_output.{sample}.txt",
    container:
        containers["crimson"]
    shell:
        """
        python3 {input.src} \
            {input.id_mappings_path} \
            --vep_txt {input.filter_vep} \
            --aln_stats_path {input.aln_stats} \
            --rna_stats_path {input.rna_stats} \
            --insert_stats_path {input.insert_stats} \
            --exon_cov_stats_path {input.exon_cov_stats} \
            --sample {wildcards.sample} \
            --vep_stats_path {input.vep_stats} > {output} 2> {log}
        """


rule multiqc:
    input:
        stats=module_output.multiqc_files,
        config=workflow.source_path("../../cfg/multiqc.yml"),
    params:
        filelist="multiqc_filelist_snv_indels.txt",
        depth=2,
        modules=multiqc_modules(),
    output:
        html="multiqc_snv_indels.html",
    log:
        "log/snv_indels.multiqc.txt",
    container:
        containers["multiqc"]
    shell:
        """
        rm -f {params.filelist}

        for fname in {input.stats}; do
            echo $fname >> {params.filelist}
        done

        multiqc \
        --force \
        --dirs \
        --dirs-depth {params.depth} \
        --fullnames \
        --fn_as_s_name \
        --file-list {params.filelist} \
        --config {input.config} \
        {params.modules} \
        --filename {output.html} 2> {log}
        """
